Required Servers
===============
You'll need 4 servers
- 1 Ceph admin node (admin)
- 3 Ceph storage nodes (node1, node2, node3)
	One dedicated disk (10G suggested) available on each
- A client node using the RADOS Block device (client0)
- An optional gateway node for running the ceph-radosgw service (gateway)

Preparing Servers to run Ceph
-----------------------------
- An admin node is set up to use ceph-deploy; from the admin node the configuration is pushed to the participating nodes.
- All nodes need to be stup for passwordless login through sudo for a specific Ceph user
- Time synchronization is mandatory
- Port 6789/tcp must be open in the firewall

Setting up Ceph Prerequisites on Centos
---------------------------------------
On the ceph admin node:
	yum -y update; reboot
	vim /etc/yum.repos.d/start-ceph.repo
		[ceph-noarch]
		name=Ceph noarch Packages
		baseurl=http://ceph.com/rpm-hammer/el7/noarch
		enabled=1
		gpgcheck=1
		type=rpm-md
		gpgkey=https://ceph.com/git/?p=ceph.git;a-blob_plain;f=keys/release.asc

	Verify the time is synchronized: timedatectl
	Add a Ceph user: useradd ceph; echo password | passwd --stdin ceph
	Configure sudo: echo "ceph ALL = (root) NOPASSWD: ALL" > /etc/sudoers.d/ceph
	chmod 0400 /etc/sudoers.d/ceph
	Check before continuing: su - ceph; sudo ls -a /root should list files
	vim /etc/ssh/sshd_config
		PasswordAuthentication yes
	Create the ceph user on all other Ceph nodes as well, and also make user SSH password login is enabled.
	ceph@admin-node$ ssh-keygen
	ceph@admin-node$ ssh-copy-id ceph@node1
	ceph@admin-node$ vim /etc/hosts
		192.168.1.1 admin-node
		192.168.1.2 node1
		192.168.1.3 node2
		192.168.1.4 node3
	ceph@admin-node$ setenforce 0; yum -y install yum-plugin-priorities
	ceph@admin-node$ yum update -y
	ceph@admin-node$ sed -i 's/requiretty/\!requiretty/' /etc/sudoers # Allow remote sudo commands to run on all nodes
	
Step 1: Deploy the Monitor Node
------------------------------
- Add steps in this procedure are performed as user ceph
ceph@admin-node$ mkdir ceph-cluster; cd ceph-cluster
ceph@admin-node$ sudo yum install -y ceph-deploy
ceph@admin-node$ ceph-deploy new <hostname>

Step 2: Install Software on all Nodes
-------------------------------------
ceph@all-node$ su - ceph
ceph@all-node$ mkdir ceph-cluster; cd ceph-cluster
ceph@all-node$ sudo yum install -y ceph-deploy
ceph@all-node$ ceph-deploy new <hostname>
ceph@all-node$ ceph-deploy install <hostname1> ... <hostnamex>
ceph@all-node$ ceph-deploy mon create-initial

Step3: Prepare OSD Nodes in the Cluster
--------------------------------------
- On all OSD nodes, make a dedicated block device available and format it with the XFS file systemc
- Make a mount point on each node, and configure persistent mounting. Note that each node will have a diffrent directory name:
ceph@osd-node$ fdisk /dev/sdb
	n
	p
	Enter
	w
ceph@osd-node$ mkfs.xfs /dev/sdb1
ceph@osd-node1$ mkdir -p /var/local/osd0
ceph@osd-node2$ mkdir -p /var/local/osd1
ceph@osd-node3$ mkdir -p /var/local/osd2
ceph@osd-node1$ echo "/dev/sdb1 /var/local/osd0 xfs noatime, nobarrier 0 0" >> /etc/fstab
ceph@osd-node2$ echo "/dev/sdb1 /var/local/osd1 xfs noatime, nobarrier 0 0" >> /etc/fstab
ceph@osd-node3$ echo "/dev/sdb1 /var/local/osd2 xfs noatime, nobarrier 0 0" >> /etc/fstab
ceph@osd-node$ mount -a: df -h

Step4: Deploy Software to the OSD Nodes
---------------------------------------
 $ ceph-deploy osd prepare node1:/var/local/osd0
 $ ceph-deploy osd prepare node2:/var/local/osd1
 $ ceph-deploy osd prepare node3:/var/local/osd2
 $ ceph-deploy osd activate node1:/var/local/osd0 node2:/var/local/osd1 node3:/var/local/osd2
 $ ceph-deploy admin cephadmin node1 node2 node3
 $ sudo chmod +r /etc/ceph/ceph/client.admin.keyring

Monitoring Status of the Cluster 
-------------------------------
ceph -s
ceph -w
ceph health
ceph health detail
