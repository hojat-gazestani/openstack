Glance Integration
------------------

controller01$ sudo yum install -y python-rbd

controller01 $ mkdir /etc/ceph
controller01 $ useradd ceph
controller01 $ passwd ceph

controller01 $ cat << EOF >/etc/sudoers.d/ceph
ceph ALL = (root) NOPASSWD:ALL
Defaults:ceph !requiretty
EOF

ceph-mon-01 $ sudo ceph osd pool create images 128

ceph-mon-01 $ sudo ceph auth get-or-create client.images mon 'allow r' osd 'allow class-read object_prefix rdb_children, allow rwx pool=images' -o /etc/ceph/ceph.client.images.keyring

ceph-mon-01 $ scp /etc/ceph/ceph.conf root@controller01:/etc/ceph

ceph-mon-01 $ chgrp glance /etc/ceph/ceph.client.images.keyring
ceph-mon-01 $ chmod 0640 /etc/ceph/ceph.client.images.keyring

ceph-mon-01 $ sudo vi /etc/ceph/ceph.conf
[client.images]
keyring = /etc/ceph/ceph.client.images.keyring

controller01 $ sudo cp /etc/glance/glance-api.conf /etc/glance/glance-api.conf.orig

controller01 $ sudo vi /etc/glance/glance-api.conf
[glance_store]
stores = glance.store.rbd.Store
default_store = rbd
rbd_store_pool = images
rbd_store_user = images
rbd_store_ceph_conf = /etc/ceph/ceph.conf

controller01 $ sudo systemctl restart openstack-glance-api

controller01 $ wget http://download.cirros-cloud.net/0.3.4/cirros-0.3.4-x86_64-disk.img
controller01 $ sudo qemu-img convert cirros-0.3.4-x86_64-disk.img cirros-0.3.4-x86_64-disk.raw
controller01 $ sudo glance image-create --name "Cirros 0.3.4" --disk-format raw --container-format bare --visibility public --file cirros-0.3.4-x86_64-disk.raw
controller01 $ sudo rbd ls images

ceph-mon-01 $ sudo rbd info images/a55e9417-67af-43c5-a342-85d2c4c483f7

Cinder Integration
------------------

ceph-mon-01 $ sudo ceph osd pool create  128
ceph-mon-01 $ sudo ceph auth get-or-create client.volumes mon 'allow r' osd 'allow class-read object_prefix rbd_children, allow rwx pool=volumes, allow rx pool=images' -o /etc/ceph/ceph.client.volumes.keyring

ceph-mon-01 $ scp /etc/ceph/ceph.client.volumes.keyring root@controller01:/etc/ceph

ceph-mon-01 $ sudo ceph auth get-key client.volumes |ssh controller01  tee client.volumes.key

controller01 $ chgrp cinder /etc/ceph/ceph.client.volumes.keyring
controller01 $ chmod 0640 /etc/ceph/ceph.client.volumes.keyring

controller01 $ vi /etc/ceph/ceph.conf
[client.volumes]
keyring = /etc/ceph/ceph.client.volumes.keyring

controller01 $ uuidgen |tee /etc/ceph/cinder.uuid.txt
controller01 $ sudo vi /etc/ceph/cinder.xml
ce6d1549-4d63-476b-afb6-88f0b196414f
client.volumes secret

controller01 $ virsh secret-define --file /etc/ceph/cinder.xml

controller01  # virsh secret-set-value --secret ce6d1549-4d63-476b-afb6-88f0b196414f --base64 $(cat /etc/ceph/client.volumes.key)
controller01  $ sudo vi /etc/cinder/cinder.conf
[rbd]
volume_driver = cinder.volume.drivers.rbd.RBDDriver
rbd_pool = volumes
rbd_ceph_conf = /etc/ceph/ceph.conf
rbd_flatten_volume_from_snapshot = false
rbd_max_clone_depth = 5
rbd_store_chunk_size = 4
rados_connect_timeout = -1
glance_api_version = 2
rbd_user = volumes
rbd_secret_uuid = ce6d1549-4d63-476b-afb6-88f0b196414f

controller01  $ openstack-service restart cinder

controller01  $ sudo cinder create --display-name="test" 1

ceph-mon-01  $ sudo rbd ls volumes
volume-d251bb74-5c5c-4c40-a15b-2a4a17bbed8b

ceph-mon-01  $ sudo rbd info volumes/volume-d251bb74-5c5c-4c40-a15b-2a4a17bbed8b


Integrating Ceph with Nova Compute
----------------------------------
ceph-mon-01  $ sudo ceph osd pool create vms 128
ceph-mon-01  $ sudo ceph auth get-or-create client.nova mon 'allow r' osd 'allow class-read object_prefix rbd_children, allow rwx pool=vms, allow rx pool=images' -o /etc/ceph/ceph.client.nova.keyring
ceph-mon-01  $ scp /etc/ceph/ceph.client.nova.keyring root@controller01:/etc/ceph

ceph-mon-01  $ sudo ceph auth get-key client.nova |ssh controller01  tee client.nova.key

controller01  # sudo chgrp nova /etc/ceph/ceph.client.nova.keyring
controller01  # sudo chmod 0640 /etc/ceph/ceph.client.nova.keyring

controller01  # sudo apt list installed python-rbd ceph-common

controller01  # sudo vim /etc/ceph/ceph.conf
[client.nova]
keyring = /etc/ceph/ceph.client.nova.keyring

controller01  # sudo uuidgen |tee /etc/ceph/nova.uuid.txt

controller01  # sudo vi /etc/ceph/nova.xml
c89c0a90-9648-49eb-b443-c97adb538f23
  
client.volumes secret

controller01  # sudo virsh secret-define --file /etc/ceph/nova.xml

controller01  # sudo virsh secret-set-value --secret c89c0a90-9648-49eb-b443-c97adb538f23 --base64 $(cat /etc/ceph/client.nova.key)

controller01  # sudo cp /etc/nova/nova.conf /etc/nova/nova.conf.orig

controller01  # sudo vi /etc/nova/nova.conf
force_raw_images = True
disk_cachemodes = writeback

[libvirt]
images_type = rbd
images_rbd_pool = vms
images_rbd_ceph_conf = /etc/ceph/ceph.conf
rbd_user = nova
rbd_secret_uuid = c89c0a90-9648-49eb-b443-c97adb538f23

controller01  # sudo systemctl restart openstack-nova-compute
controller01  # sudo neutron net-list
controller01  # sudo nova boot --flavor m1.small --nic net-id=4683d03d-30fc-4dd1-9b5f-eccd87340e70 --image='Cirros 0.3.4' cephvm
controller01  # sudo nova list

ceph-mon-01  $ sudo rbd -p vms ls

Troubleshooting
---------------
controller01  ceph(keystone_admin)]# nova image-list

  
[root@controller01  ceph(keystone_admin)]# rbd -p images snap unprotect cf56345e-1454-4775-84f6-781912ce242b@snap
[root@controller01  ceph(keystone_admin)]# rbd -p images snap rm cf56345e-1454-4775-84f6-781912ce242b@snap
[root@controller01  ceph(keystone_admin)]# glance image-delete cf56345e-1454-4775-84f6-781912ce242b

Source:
https://superuser.openstack.org/articles/ceph-as-storage-for-openstack/

-----------------------------------------------------------------------------------------
# I didn't use this one yet.

ceph osd pool create volumes
ceph osd pool create images
ceph osd pool create backups
ceph osd pool create vms

rbd pool init volumes
rbd pool init images
rbd pool init backups
rbd pool init vms

The nodes running glance-api, cinder-volume, nova-compute and cinder-backup act as Ceph clients. Each requires the ceph.conf file

ssh {your-openstack-server} sudo tee /etc/ceph/ceph.conf </etc/ceph/ceph.conf

sudo apt-get install python-rbd
sudo apt-get install ceph-common

ceph auth get-or-create client.glance mon 'profile rbd' osd 'profile rbd pool=images' mgr 'profile rbd pool=images'
ceph auth get-or-create client.cinder mon 'profile rbd' osd 'profile rbd pool=volumes, profile rbd pool=vms, profile rbd-read-only pool=images' mgr 'profile rbd pool=volumes, profile rbd pool=vms'
ceph auth get-or-create client.cinder-backup mon 'profile rbd' osd 'profile rbd pool=backups' mgr 'profile rbd pool=backups'

ceph auth get-or-create client.glance | ssh {your-glance-api-server} sudo tee /etc/ceph/ceph.client.glance.keyring
ssh {your-glance-api-server} sudo chown glance:glance /etc/ceph/ceph.client.glance.keyring
ceph auth get-or-create client.cinder | ssh {your-volume-server} sudo tee /etc/ceph/ceph.client.cinder.keyring
ssh {your-cinder-volume-server} sudo chown cinder:cinder /etc/ceph/ceph.client.cinder.keyring
ceph auth get-or-create client.cinder-backup | ssh {your-cinder-backup-server} sudo tee /etc/ceph/ceph.client.cinder-backup.keyring
ssh {your-cinder-backup-server} sudo chown cinder:cinder /etc/ceph/ceph.client.cinder-backup.keyring

ceph auth get-or-create client.cinder | ssh {your-nova-compute-server} sudo tee /etc/ceph/ceph.client.cinder.keyring

ceph auth get-key client.cinder | ssh {your-compute-node} tee client.cinder.key

compute-node
	uuidgen
	457eb676-33da-42ec-9a8c-9293d545c337

	cat > secret.xml <<EOF
	<secret ephemeral='no' private='no'>
	  <uuid>457eb676-33da-42ec-9a8c-9293d545c337</uuid>
	  <usage type='ceph'>
	    <name>client.cinder secret</name>
	  </usage>
	</secret>
	EOF
	sudo virsh secret-define --file secret.xml
	Secret 457eb676-33da-42ec-9a8c-9293d545c337 created
	sudo virsh secret-set-value --secret 457eb676-33da-42ec-9a8c-9293d545c337 --base64 $(cat client.cinder.key) && rm client.cinder.key secret.xml

/etc/glance/glance-api.conf
	[glance_store]
	stores = rbd
	default_store = rbd
	rbd_store_pool = images
	rbd_store_user = glance
	rbd_store_ceph_conf = /etc/ceph/ceph.conf
	rbd_store_chunk_size = 8

	show_image_direct_url = True

/var/lib/glance/image-cache/
	flavor = keystone+cachemanagement


	[paste_deploy]
	flavor = keystone

	hw_scsi_model=virtio-scsi
	hw_disk_bus=scsi
	hw_qemu_guest_agent=yes
	os_require_quiesce=yes

/etc/cinder/cinder.conf
	[DEFAULT]
	...
	enabled_backends = ceph
	glance_api_version = 2
	...
	[ceph]
	volume_driver = cinder.volume.drivers.rbd.RBDDriver
	volume_backend_name = ceph
	rbd_pool = volumes
	rbd_ceph_conf = /etc/ceph/ceph.conf
	rbd_flatten_volume_from_snapshot = false
	rbd_max_clone_depth = 5
	rbd_store_chunk_size = 4
	rados_connect_timeout = -1

	[ceph]
	...
	rbd_user = cinder
	rbd_secret_uuid = 457eb676-33da-42ec-9a8c-9293d545c337

sudo vim /etc/cinder/cinder.conf
backup_driver = cinder.backup.drivers.ceph
backup_ceph_conf = /etc/ceph/ceph.conf
backup_ceph_user = cinder-backup
backup_ceph_chunk_size = 134217728
backup_ceph_pool = backups
backup_ceph_stripe_unit = 0
backup_ceph_stripe_count = 0
restore_discard_excess_bytes = true

[libvirt]
...
rbd_user = cinder
rbd_secret_uuid = 457eb676-33da-42ec-9a8c-9293d545c337

(Nova compute) node:
ceph daemon /var/run/ceph/ceph-client.cinder.19195.32310016.asok help

sudo vim ceph.conf
[client]
    rbd cache = true
    rbd cache writethrough until flush = true
    admin socket = /var/run/ceph/guests/$cluster-$type.$id.$pid.$cctid.asok
    log file = /var/log/qemu/qemu-guest-$pid.log
    rbd concurrent management ops = 20

mkdir -p /var/run/ceph/guests/ /var/log/qemu/
chown qemu:libvirtd /var/run/ceph/guests /var/log/qemu/

sudo glance-control api restart
sudo service nova-compute restart
sudo service cinder-volume restart
sudo service cinder-backup restart

cinder create --image-id {id of image} --display-name {name of volume} {size of volume}

qemu-img convert -f {source-format} -O {output-format} {source-filename} {output-filename}
qemu-img convert -f qcow2 -O raw precise-cloudimg.img precise-cloudimg.raw


Source:
https://docs.ceph.com/en/latest/rbd/rbd-openstack/



